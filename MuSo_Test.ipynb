{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MuSo Test\n",
    "MuSo is a supervised machine learning model for predicting user compatibility (degrees of separation) based on their music preferences. This document handles loading the model and running inference tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to LastFM successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import pylast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.typing import NDArray\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_NAME = \"MuSo_User\"\n",
    "TEST_DATASET_NAME = \"MuSo_Test_User\"\n",
    "\n",
    "def load_credentials_json_from_file():\n",
    "    primary_path = Path(\"Credentials.json\")\n",
    "    fallback_path = Path(\"LastFM_Credentials.json\")\n",
    "    if primary_path.exists():\n",
    "        file_path = primary_path\n",
    "    else:\n",
    "        file_path = fallback_path\n",
    "    with file_path.open('r') as file:\n",
    "        data = json.load(file)\n",
    "    if not all(list(data.values())):\n",
    "        raise ValueError(f\"The file '{file_path}' MUST have all values defined.\")\n",
    "    return data\n",
    "\n",
    "credentials = load_credentials_json_from_file()\n",
    "\n",
    "try:\n",
    "    lastfm = pylast.LastFMNetwork(\n",
    "        api_key=credentials[\"LASTFM_API_KEY\"],\n",
    "        api_secret=credentials[\"LASTFM_API_SECRET\"],\n",
    "        username=credentials[\"LASTFM_USERNAME\"],\n",
    "        password_hash=pylast.md5(credentials[\"LASTFM_PASSWORD\"]),\n",
    "    )\n",
    "    print(\"Connected to LastFM successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to LastFM: {str(e)}\")\n",
    "\n",
    "with open(f\"{DATASET_NAME}_Data.json\", \"r\") as f:\n",
    "    USER_DATA:dict = json.load(f)\n",
    "\n",
    "USER_DOS = pd.read_csv(f\"{DATASET_NAME}_DOS.csv\", index_col = 0)\n",
    "MAX_DOS = USER_DOS.max().max()\n",
    "TEST_USER_DOS = pd.read_csv(f\"{TEST_DATASET_NAME}_DOS.csv\", index_col = 0)\n",
    "    \n",
    "if \"top_tags\" in list(USER_DATA.values())[0]:\n",
    "    INCL_TAGS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERIOD = pylast.PERIOD_6MONTHS\n",
    "GULD_LIM = 100\n",
    "TAGS_ARTISTS_LIM = 10\n",
    "\n",
    "extract_n_items = lambda d, n: dict(list(d.items())[:n])\n",
    "\n",
    "def remove_missing_data(user_list_data:dict[str, dict[str, dict[str, int]]]):\n",
    "    data = {user: data for user, data in user_list_data.items() if all(data.values())}\n",
    "    return data\n",
    "\n",
    "def get_user_list_data(\n",
    "        user_list:list[str],\n",
    "        period:str=pylast.PERIOD_6MONTHS,\n",
    "        artists_limit:int|None=None,\n",
    "        albums_limit:int|None=None,\n",
    "        tracks_limit:int|None=None,\n",
    "        include_tags:bool=False,\n",
    "        tags_limit:int|None=None,\n",
    "        tags_artists_limit:int|None=None,\n",
    "        cache = True,\n",
    "    ):\n",
    "    cache_path = Path(\"User_Data_Cache.json\")\n",
    "    if cache:\n",
    "        cached_udata = {}\n",
    "        if cache_path.exists():\n",
    "            with cache_path.open(\"r\") as f:\n",
    "                cached_udata.update(json.load(f))\n",
    "        else:\n",
    "            dataset_path = Path(f\"{DATASET_NAME}_Data.json\")\n",
    "            test_dataset_path = Path(f\"{TEST_DATASET_NAME}_Data.json\")\n",
    "            if dataset_path.exists():\n",
    "                with dataset_path.open(\"r\") as f:\n",
    "                    cached_udata.update(json.load(f))\n",
    "            if test_dataset_path.exists():\n",
    "                with test_dataset_path.open(\"r\") as f:\n",
    "                    cached_udata.update(json.load(f))\n",
    "        data = {u:udata for u, udata in cached_udata.items() if u in user_list}\n",
    "        user_list = [u for u in user_list if u not in cached_udata]\n",
    "    else:\n",
    "        if cache_path.exists():\n",
    "            cache_path.unlink()\n",
    "        data = {}\n",
    "\n",
    "    extract_name_plays = lambda top_items: {t.item.get_name() : int(t.weight) for t in top_items}\n",
    "    for user in user_list:\n",
    "        user = lastfm.get_user(user)\n",
    "        top_artists = user.get_top_artists(period=period, limit=artists_limit)\n",
    "        top_albums = user.get_top_albums(period=period, limit=albums_limit)\n",
    "        top_tracks = user.get_top_tracks(period=period, limit=tracks_limit)\n",
    "        user_name = user.get_name()\n",
    "        data[user_name] = {}\n",
    "        data[user_name][\"top_artists\"] = extract_name_plays(top_artists)\n",
    "        data[user_name][\"top_albums\"] = extract_name_plays(top_albums)\n",
    "        data[user_name][\"top_tracks\"] = extract_name_plays(top_tracks)\n",
    "        if include_tags:\n",
    "            num_artists = 0\n",
    "            tags = {}\n",
    "            for top_artist in top_artists:\n",
    "                top_tags = top_artist.item.get_top_tags()\n",
    "                artist_weight = int(top_artist.weight)\n",
    "                for top_tag in top_tags:\n",
    "                    tag = top_tag.item.get_name()\n",
    "                    tag_weight = int(top_tag.weight) * artist_weight\n",
    "                    if tag in tags:\n",
    "                        tags[tag] += tag_weight\n",
    "                    else:\n",
    "                        tags[tag] = tag_weight\n",
    "                if tags_artists_limit:\n",
    "                    num_artists += 1\n",
    "                    if num_artists >= tags_artists_limit:\n",
    "                        break\n",
    "            data[user_name][\"top_tags\"] = extract_n_items(tags, tags_limit) \n",
    "    if cache:\n",
    "        cached_udata.update(remove_missing_data(data))\n",
    "        with cache_path.open(\"w\") as f:\n",
    "            json.dump(cached_udata, f)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_user_plays(all_item_list:list, udata:dict, user:str, top_item:str):\n",
    "    user_value_list = []\n",
    "    for item in all_item_list:\n",
    "        if item in udata[user][top_item]:\n",
    "            user_value_list.append(udata[user][top_item][item])\n",
    "        else:\n",
    "            user_value_list.append(0)\n",
    "    return np.array(user_value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_top_items(data:dict, top_item:str, shuffled:bool=False)->list[str]:\n",
    "    all_items = []\n",
    "    for user in data:\n",
    "        all_items += list(data[user][top_item].keys())\n",
    "        \n",
    "    all_unique_items = list(set(all_items))\n",
    "    if shuffled:\n",
    "        random.shuffle(all_unique_items)\n",
    "    else:\n",
    "        all_unique_items.sort()\n",
    "    return all_unique_items\n",
    "\n",
    "def get_all_unique_top_items(data:dict, include_tags:bool=False):\n",
    "    do_shuffle = False\n",
    "    all_artists = get_unique_top_items(data, \"top_artists\", shuffled=do_shuffle)\n",
    "    all_albums = get_unique_top_items(data, \"top_albums\", shuffled=do_shuffle)\n",
    "    all_tracks = get_unique_top_items(data, \"top_tracks\", shuffled=do_shuffle)\n",
    "    if include_tags:\n",
    "        all_tags = get_unique_top_items(data, \"top_tags\", shuffled=do_shuffle)\n",
    "        random.shuffle(all_tags)\n",
    "    else:\n",
    "        all_tags = None\n",
    "    return all_artists, all_albums, all_tracks, all_tags\n",
    "\n",
    "def normalize(a:NDArray[np.int64]):\n",
    "    return (a - a.mean()) / a.std()\n",
    "\n",
    "def get_user_vector(user1:str, user2:str):\n",
    "    all_artists, all_albums, all_tracks, all_tags = get_all_unique_top_items(USER_DATA, INCL_TAGS)\n",
    "    ulist = [user1, user2]\n",
    "    udata = get_user_list_data(ulist, PERIOD, GULD_LIM, GULD_LIM, GULD_LIM, INCL_TAGS, GULD_LIM, TAGS_ARTISTS_LIM)\n",
    "    udata = remove_missing_data(udata)\n",
    "    assert user1 in udata and user2 in udata\n",
    "    u1_all_artists_vec = normalize(replace_with_user_plays(all_artists, udata, user1, \"top_artists\"))\n",
    "    u1_all_albums_vec = normalize(replace_with_user_plays(all_albums, udata, user1, \"top_albums\"))\n",
    "    u1_all_tracks_vec = normalize(replace_with_user_plays(all_tracks, udata, user1, \"top_tracks\"))\n",
    "    u2_all_artists_vec = normalize(replace_with_user_plays(all_artists, udata, user2, \"top_artists\"))\n",
    "    u2_all_albums_vec = normalize(replace_with_user_plays(all_albums, udata, user2, \"top_albums\"))\n",
    "    u2_all_tracks_vec = normalize(replace_with_user_plays(all_tracks, udata, user2, \"top_tracks\"))\n",
    "    artist_vec = normalize(np.hstack([u1_all_artists_vec, u2_all_artists_vec]))\n",
    "    album_vec = normalize(np.hstack([u1_all_albums_vec, u2_all_albums_vec]))\n",
    "    track_vec = normalize(np.hstack([u1_all_tracks_vec, u2_all_tracks_vec]))\n",
    "    vec_list = [artist_vec, album_vec, track_vec]\n",
    "    if INCL_TAGS:\n",
    "        assert all_tags\n",
    "        u1_all_tags_vec = normalize(replace_with_user_plays(all_tags, udata, user1, \"top_tags\"))\n",
    "        u2_all_tags_vec = normalize(replace_with_user_plays(all_tags, udata, user2, \"top_tags\"))\n",
    "        tag_vec = normalize(np.hstack([u1_all_tags_vec, u2_all_tags_vec]))\n",
    "        vec_list.append(tag_vec)\n",
    "    #random.shuffle(vec_list)\n",
    "    return normalize(np.hstack(vec_list)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SeparationModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SeparationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 8192)\n",
    "        self.fc2 = nn.Linear(8192, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 2048)\n",
    "        self.fc4 = nn.Linear(2048, 1024)\n",
    "        self.fc5 = nn.Linear(1024, 512)\n",
    "        self.fc6 = nn.Linear(512, 256)\n",
    "        self.fc7 = nn.Linear(256, 128)\n",
    "        self.fc8 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc8(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "MODEL_PATH = \"Muso.pth\"\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "device = torch.device(DEVICE)\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = torch.load(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def perform_inference(model, input_tensor):\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    return probabilities, logits\n",
    "\n",
    "MODEL = load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Degree of Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree of Separation:  1\n"
     ]
    }
   ],
   "source": [
    "def predict_dos(user1, user2):\n",
    "    user_vec = get_user_vector(user1, user2)\n",
    "    assert len(user_vec) == 18432\n",
    "    user_tensor = torch.from_numpy(user_vec).unsqueeze(0).to(device)\n",
    "    probabilities, logits = perform_inference(MODEL, user_tensor)\n",
    "    _, predicted_class = torch.max(logits, dim=1)\n",
    "    return int(predicted_class.item() + 1)\n",
    "\n",
    "print(\"Degree of Separation: \", predict_dos(\"fshnoeyes\", \"larrywalker27\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\holde\\AppData\\Local\\Temp\\ipykernel_23884\\4214944316.py:26: RuntimeWarning: invalid value encountered in divide\n",
      "  return (a - a.mean()) / a.std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Predictions:  350\n",
      "Model Accuracy:  17.142857142857142\n",
      "Random Guess Accuracy:  11.428571428571429\n"
     ]
    }
   ],
   "source": [
    "TEST_USERS = list(TEST_USER_DOS.columns)\n",
    "num_pred = 0\n",
    "correct_pred = 0\n",
    "correct_rand_pred = 0\n",
    "for user1 in TEST_USERS:\n",
    "    for user2 in TEST_USERS:\n",
    "        if user1 != user2:\n",
    "            if user1 not in USER_DATA and user2 not in USER_DATA:\n",
    "                true_dos = TEST_USER_DOS.at[user1, user2]\n",
    "                if true_dos != 0 and true_dos <= MAX_DOS:\n",
    "                    num_pred += 1\n",
    "                    pred_dos = predict_dos(user1, user2)\n",
    "                    if abs(pred_dos - true_dos) <= 3:\n",
    "                        correct_pred += 1\n",
    "                    if abs(random.randint(1, MAX_DOS) - true_dos) <= 3:\n",
    "                        correct_rand_pred += 1\n",
    "print(\"Number of Predictions: \", num_pred)\n",
    "print(\"Model Accuracy: \", (correct_pred / num_pred) * 100)\n",
    "print(\"Random Guess Accuracy: \", (correct_rand_pred / num_pred) * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
